'''
peft
transformers
bitsandbytes
ipykernel
rapidfuzz
datasets
gradio
sentencepiece
vllm
openai

pip install peft transformers bitsandbytes ipykernel rapidfuzz datasets gradio sentencepiece vllm openai

A4000 x 2

python -m vllm.entrypoints.openai.api_server --model svjack/Genshin_Impact_Qwen_1_5_Plot_Chat_roleplay_chat_AWQ --dtype auto \
 --api-key token-abc123 --tensor-parallel-size 2 --quantization awq --max-model-len 4000 --gpu-memory-utilization 0.35 --port 8000

python -m vllm.entrypoints.openai.api_server --model svjack/DPO_Genshin_Impact_Qwen_1_5_Plot_Engine_Step_Json_Short_AWQ --dtype auto \
 --api-key token-abc123 --tensor-parallel-size 2 --quantization awq --max-model-len 2000 --gpu-memory-utilization 0.35 --port 8001
'''

import gradio as gr

from transformers import TextStreamer, TextIteratorStreamer, AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

import pandas as pd
import json
import numpy as np
import re
from datasets import Dataset, load_dataset
from rapidfuzz import fuzz
from threading import Thread

from openai import OpenAI

from typing import List, Optional, Tuple, Dict
History = List[Tuple[str, str]]
Messages = List[Dict[str, str]]

adapter_name_client_dict = {
    "chat": {
    "model": "svjack/Genshin_Impact_Qwen_1_5_Plot_Chat_roleplay_chat_AWQ",
    "client": OpenAI(
        api_key="token-abc123",
        base_url="http://localhost:8000/v1",
    )},
    "engine": {
    "model": "svjack/DPO_Genshin_Impact_Qwen_1_5_Plot_Engine_Step_Json_Short_AWQ",
    "client": OpenAI(
        api_key="token-abc123",
        base_url="http://localhost:8001/v1",
    )}
}

def openai_predict(messages,
    adapter_name = "chat",
    temperature = 0.01
    ):
    stream = adapter_name_client_dict[adapter_name]["client"].chat.completions.create(
            model=adapter_name_client_dict[adapter_name]["model"],  # Model name to use
            messages=messages,  # Chat history
            temperature=temperature,  # Temperature for text generation
            stream=True,  # Stream response
    )
    # Read and return generated text from response stream
    partial_message = ""
    for chunk in stream:
        #clear_output(wait = True)
        partial_message += (chunk.choices[0].delta.content or "")
        #print(partial_message)
    return partial_message
    #return out

def history_to_messages(history: History, system: str) -> Messages:
    messages = [{'role': "system", 'content': system}]
    for h in history:
        messages.append({'role': "user", 'content': h[0]})
        messages.append({'role': "assistant", 'content':
            h[1]
        })
    return messages

def messages_to_history(messages: Messages) -> Tuple[str, History]:
    assert messages[0]['role'] == "system"
    system = messages[0]['content']
    history = []
    import numpy as np
    import pandas as pd
    from copy import deepcopy
    messages = deepcopy(messages)

    messages_ = []
    for ele in messages[1:]:
        if not messages_:
            messages_.append(ele)
        else:
            if messages_[-1]["role"] == ele["role"]:
                continue
            else:
                messages_.append(ele)

    last_message = messages_[-1]
    last_role = last_message["role"]
    if last_role == "user":
        messages_.append(
            {
                "role": "assistant",
                "content": ""
            }
        )
    history = pd.DataFrame(np.asarray(messages_).reshape([-1, 2]).tolist()).applymap(
        lambda x: x["content"]
    ).applymap(
        lambda x: x
    ).values.tolist()
    return system, history

def openai_predict_stream(messages,
    temperature = 0.01,
    adapter_name = "chat"
    ):
    stream = adapter_name_client_dict[adapter_name]["client"].chat.completions.create(
            model=adapter_name_client_dict[adapter_name]["model"],  # Model name to use
            messages=messages,  # Chat history
            temperature=temperature,  # Temperature for text generation
            stream=True,  # Stream response
    )

    system, history = messages_to_history(messages)

    # Read and return generated text from response stream
    partial_message = ""
    for chunk in stream:
        #clear_output(wait = True)
        partial_message += (chunk.choices[0].delta.content or "")
        #print(partial_message)
        history[-1][1] = partial_message
        yield system, history

    '''
    # Initialize an empty string to store the generated text
    partial_text = ""
    for new_text in streamer:
        partial_text += new_text.split("<|im_start|>assistant")[-1].replace("<|im_end|>", "").strip()
        history[-1][1] = partial_text
        yield system, history
    '''

def run_step_infer_times(x, times = 5, temperature = 0.01,
                        repetition_penalty = 1.0,
                        sim_val = 70,
                        adapter_name = "engine"
                        ):
    req = []
    for _ in range(times):
        #clear_output(wait = True)
        '''
        out = qwen_hf_predict([
                {
                    "role": "system",
                    "content": ""
                },
                {
                    "role": "user",
                    "content": x
                },
            ],
            repetition_penalty = repetition_penalty,
            temperature = temperature,
            max_new_tokens = 2070,
            max_input_length = 6000,
            adapter_name = adapter_name
        )
        '''
        out = openai_predict([
                {
                    "role": "system",
                    "content": ""
                },
                {
                    "role": "user",
                    "content": x
                },
            ],
            temperature = temperature,
            adapter_name = adapter_name
        )
        if req:
            val = max(map(lambda x: fuzz.ratio(x, out), req))
            #print(val)
            #print(req)
            if val < sim_val:
                req.append(out.strip())
            x = x.strip() + "\n" + out.strip()
        else:
            req.append(out.strip())
            x = x.strip() + "\n" + out.strip()
    return req

plot_summary_df = load_dataset("svjack/Genshin-Impact-Plot-Summary")["train"].to_pandas()
character_portrait_df = load_dataset("svjack/Genshin-Impact-Plot-Character-Portrait-Merged")["train"].to_pandas()
#character_portrait_df = load_dataset("svjack/Genshin-Impact-Plot-Character-Portrait")["train"].to_pandas()
del character_portrait_df["portrait"]
plot_summary_df = plot_summary_df.dropna().drop_duplicates()
character_portrait_df = character_portrait_df.dropna().drop_duplicates()

name_list = character_portrait_df["name"].dropna().drop_duplicates().values.tolist()
title_list = plot_summary_df["title"].dropna().drop_duplicates().values.tolist()

ll0 = ['æ•é£çš„å¼‚ä¹¡äºº',
 'ä¸ºäº†æ²¡æœ‰çœ¼æ³ªçš„æ˜å¤©',
 'å·¨é¾™ä¸è‡ªç”±ä¹‹æ­Œ',
 'æµ®ä¸–æµ®ç”Ÿåƒå²©é—´',
 'è¾è¡Œä¹…è¿œä¹‹èº¯',
 'è¿«è¿‘çš„å®¢æ˜Ÿ',
 'æˆ‘ä»¬ç»ˆå°†é‡é€¢',
 'æŒ¯è¢–ç§‹é£é—®çº¢å¶',
 'ä¸åŠ¨é¸£ç¥ï¼Œæ’å¸¸ä¹åœŸ',
 'æ— å¿µæ— æƒ³ï¼Œæ³¡å½±æ–­ç­',
 'åƒæ‰‹ç™¾çœ¼ï¼Œå¤©ä¸‹äººé—´',
 'å›å“æ¸Šåº•çš„å®‰é­‚æ›²',
 'ç©¿è¶ŠçƒŸå¸·ä¸æš—æ—',
 'åƒæœµç«ç‘°å¸¦æ¥çš„é»æ˜',
 'è¿·æ¢¦ä¸ç©ºå¹»ä¸æ¬ºéª—',
 'èµ¤åœŸä¹‹ç‹ä¸ä¸‰æœåœ£è€…',
 'è™šç©ºé¼“åŠ¨ï¼ŒåŠ«ç«é«˜æ‰¬',
 'å¡åˆ©è´å°”',
 'é£èµ·é¹¤å½’',
 'å±é€”ç–‘è¸ª',
 'å€¾è½ä¼½è“']

ll1 = ['æµ·ç›—ç§˜å®',
 'é£ã€å‹‡æ°”å’Œç¿…è†€',
 'éº»çƒ¦çš„å·¥ä½œ',
 'æš—å¤œè‹±é›„çš„ä¸åœ¨åœºè¯æ˜',
 'å¢çš®å¡çš„æ„ä¹‰',
 'çœŸæ­£çš„å®ç‰©',
 'éª‘å£«å›¢é•¿çš„ä¸€æ—¥å‡æœŸ',
 'è‹¥ä½ å›°äºæ— é£ä¹‹åœ°',
 'åœ¨æ­¤ä¸–çš„æ˜Ÿç©ºä¹‹å¤–',
 'æ—…è¡Œè€…è§‚å¯ŸæŠ¥å‘Š',
 'æµªèŠ±ä¸å†å½’æµ·',
 'è’™å¾·é£Ÿé‡ä¹‹æ—…',
 'æ±Ÿæ¹–ä¸é—®å‡ºå¤„',
 'ç›èŠ±',
 'åŒªçŸ³',
 'äº‘ä¹‹æµ·ï¼Œäººä¹‹æµ·',
 'æ§æŸ¯èƒ¡è¶ï¼Œå‚©ä½‘ä¹‹æ¢¦',
 'å¥ˆä½•è¶é£å»',
 'æ£‹ç”Ÿæ–­å¤„',
 'ã€ŒåŒ»å¿ƒã€',
 'é¹¤ä¸ç™½å…”çš„è¯‰è¯´',
 'å¦‚æ¢¦å¦‚ç”µçš„éš½æ°¸',
 'å½±ç…§æµ®ä¸–é£æµ',
 'é¡»è‡¾ç™¾æ¢¦',
 'å…µæˆˆæ¢¦å»ï¼Œæ˜¥è‰å¦‚èŒµ',
 'èµ¤é‡‘é­‚',
 'é¸£ç¥å¾¡ç¥“ç¥ˆæ„¿ç¥­',
 'æ¢§æ¡ä¸€å¶è½',
 'é™Œé‡ä¸è¯†æ•…äºº',
 'æ‹¾æ˜Ÿä¹‹æ—…',
 'å½“ä»–ä»¬è°ˆèµ·ä»Šå¤œ',
 'æ²¡æœ‰ç­”æ¡ˆçš„è¯¾é¢˜',
 'æ²‰æ²™å½’å¯‚',
 'è‡´æ™ºæ…§è€…',
 'ä½™æ¸©',
 'å½’ä¹¡',
 'ä¹Œåˆçš„è™šåƒ',
 'ã€Œç‹®ä¹‹è¡€ã€',
 'è¢«é—å¿˜çš„æ€ªç›—',
 'å¾€æ—¥ç•™ç—•']

assert all(map(lambda x: x in title_list, ll0))
assert all(map(lambda x: x in title_list, ll1))
title_list = ll0 + ll1

def find_plot_info(title):
    return dict(filter(lambda t2: t2[1] ,plot_summary_df[
        plot_summary_df["title"] == title
    ][[
     'æ•…äº‹èƒŒæ™¯b',
     'äº‹ä»¶èµ·å› b',
     'äº‹ä»¶ç»è¿‡b',
     'äº‹ä»¶åè½¬b',
     'äº‹ä»¶ç»“æŸb',
     'äº‹ä»¶æ„ä¹‰b',
     'åç»­å‰§æƒ…b'
    ]].rename(
        columns = {
     'æ•…äº‹èƒŒæ™¯b': "æ•…äº‹èƒŒæ™¯",
     'äº‹ä»¶èµ·å› b': "äº‹ä»¶èµ·å› ",
     'äº‹ä»¶ç»è¿‡b': "äº‹ä»¶ç»è¿‡",
     'äº‹ä»¶åè½¬b': "äº‹ä»¶åè½¬",
     'äº‹ä»¶ç»“æŸb': "äº‹ä»¶ç»“æŸ",
     'äº‹ä»¶æ„ä¹‰b': "äº‹ä»¶æ„ä¹‰",
     'åç»­å‰§æƒ…b': "åç»­å‰§æƒ…"
        }
    ).T.apply(
        lambda x: sorted(set(
            map(lambda z: z.strip() ,filter(lambda y: y.strip() ,x.tolist()))
        ), key = len, reverse = True), axis = 1
    ).to_dict().items()))

#### top_k ä¸åº”è¯¥è¿‡åº¦é™åˆ¶è§’è‰²æ•°é‡ï¼Œå¦‚æœæ˜¯å…³é”®è§’è‰²
def pick_names_from_summary(summary, name_list, top_k = 6):
    repeat_l = ["è‡ªå·±", "è§", "ç©º", "å½¼æ­¤", "å¤©å›", "å¤œå‰", "å½±"]
    name_list_in_x = list(filter(lambda y: y in summary and y not in repeat_l, name_list))
    return sorted(name_list_in_x, key = lambda x: len(summary.split(x)), reverse = True)[:top_k]

def build_instruction_for_plot_engine(title ,Summary, name_list_in_x):
    instruction = "\n".join(map(lambda y: y.strip() ,filter(lambda x: x.strip() ,'''
        æ•…äº‹æ ‡é¢˜:{title}
        æ•…äº‹èƒŒæ™¯:{Summary}
        å‚ä¸è§’è‰²:{all_candidates}
        '''.format(
            **{
                "title": title,
                "Summary": Summary.replace("\n", "").replace(" ", ""),
                "all_candidates": "ã€".join(name_list_in_x),
            }
        ).split("\n"))))
    return instruction

def build_chat_system_head(total_background, now_background, Person1, Person2, character_portrait_df):
    system_head = '''
    æ•…äº‹èƒŒæ™¯:{total_background}
    å½“å‰æ•…äº‹èƒŒæ™¯:{now_background}
    å‚ä¸è€…1:{Person1}
    å‚ä¸è€…1è§’è‰²ç»å†:{Person1_hist}
    å‚ä¸è€…1æ€§æ ¼ç‰¹å¾:{Person1_char}
    å‚ä¸è€…1å‰§æƒ…ä¸­çš„ä½œç”¨:{Person1_plot}
    å‚ä¸è€…2:{Person2}
    å‚ä¸è€…2è§’è‰²ç»å†:{Person2_hist}
    å‚ä¸è€…2æ€§æ ¼ç‰¹å¾:{Person2_char}
    å‚ä¸è€…2å‰§æƒ…ä¸­çš„ä½œç”¨:{Person2_plot}
    è¦æ±‚è¿›è¡Œ"{Person1}"ä¸"{Person2}"ä¹‹é—´çš„å¯¹è¯ã€‚
    æˆ‘æ‰®æ¼”"{Person1}"ï¼Œä½ æ‰®æ¼”"{Person2}"ã€‚
    '''.format(
        **{
            "total_background": total_background.replace("\n", "").replace(" ", ""),
            "now_background": now_background.replace("\n", "").replace(" ", ""),
            "Person1":Person1,
            "Person1_hist": dict(character_portrait_df[["name", "è§’è‰²ç»å†"]].values.tolist()).get(Person1, "").replace("\n", "").replace(" ", ""),
            "Person1_char": dict(character_portrait_df[["name", "æ€§æ ¼ç‰¹å¾"]].values.tolist()).get(Person1, "").replace("\n", "").replace(" ", ""),
            "Person1_plot": dict(character_portrait_df[["name", "å‰§æƒ…ä¸­çš„ä½œç”¨"]].values.tolist()).get(Person1, "").replace("\n", "").replace(" ", ""),
            "Person2":Person2,
            "Person2_hist": dict(character_portrait_df[["name", "è§’è‰²ç»å†"]].values.tolist()).get(Person2, "").replace("\n", "").replace(" ", ""),
            "Person2_char": dict(character_portrait_df[["name", "æ€§æ ¼ç‰¹å¾"]].values.tolist()).get(Person2, "").replace("\n", "").replace(" ", ""),
            "Person2_plot": dict(character_portrait_df[["name", "å‰§æƒ…ä¸­çš„ä½œç”¨"]].values.tolist()).get(Person2, "").replace("\n", "").replace(" ", ""),
        }
    )
    system_head = "\n".join(map(lambda y: y.strip() ,filter(lambda x: x.strip() ,system_head.split("\n"))))
    return system_head

def run_step_infer_times_run(x, times = 5, temperature = 0.01,
                        repetition_penalty = 1.0,
                        sim_val = 70,
                        name_list_in_x = [],
                        adapter_name = "engine"
                        ):
    req = []
    for _ in range(times):
        #clear_output(wait = True)
        out = openai_predict([
                {
                    "role": "system",
                    "content": ""
                },
                {
                    "role": "user",
                    "content": x
                },
            ],
            temperature = temperature,
            adapter_name = adapter_name
        )
        try:
            d = eval(out)
            assert len(d) == 3
            assert "å‚ä¸è€…1" in d and "å‚ä¸è€…2" in d and "å½“å‰æ•…äº‹èƒŒæ™¯" in d
            d["å½“å‰æ•…äº‹èƒŒæ™¯"] = d["å½“å‰æ•…äº‹èƒŒæ™¯"].replace("\n", "").replace(" ", "")
            out = str(d)
        except:
            print("parse error")
            continue
        if req:
            val = max(map(lambda x: fuzz.ratio(x, out), req))
            if val < sim_val:
                if d["å‚ä¸è€…1"] != d["å‚ä¸è€…2"]:
                    req.append(out.strip())
                    req_iter = list(map(eval ,req))
                    yield "\n".join(map(lambda d:
                        "\n".join(map(lambda t2: "{}:{}".format(t2[0], t2[1]) ,d.items()))
                     ,req_iter))
                else:
                    if name_list_in_x:
                        l = list(
                        filter(lambda ele: ele not in [d["å‚ä¸è€…1"], d["å‚ä¸è€…2"]] and ele in d["å½“å‰æ•…äº‹èƒŒæ™¯"],
                        name_list_in_x)
                        )
                        if l:
                            sc = d["å½“å‰æ•…äº‹èƒŒæ™¯"]
                            rp = d["å½“å‰æ•…äº‹èƒŒæ™¯"].replace(d["å‚ä¸è€…2"], l[0])
                            assert sc in out
                            out = out.replace(sc, rp)
                            req.append(out.strip())
                            req_iter = list(map(eval ,req))
                            req_iter = list(filter(lambda x: type(x) == type({}), req_iter))
                            yield "\n".join(map(lambda d:
                                "\n".join(map(lambda t2: "{}:{}".format(t2[0], t2[1]) ,d.items()))
                             ,req_iter))
        else:
            if d["å‚ä¸è€…1"] != d["å‚ä¸è€…2"]:
                req.append(out.strip())
                req_iter = list(map(eval ,req))
                req_iter = list(filter(lambda x: type(x) == type({}), req_iter))
                yield "\n".join(map(lambda d:
                    "\n".join(map(lambda t2: "{}:{}".format(t2[0], t2[1]) ,d.items()))
                 ,req_iter))
            else:
                if name_list_in_x:
                    l = list(
                        filter(lambda ele: ele not in [d["å‚ä¸è€…1"], d["å‚ä¸è€…2"]] and ele in d["å½“å‰æ•…äº‹èƒŒæ™¯"],
                        name_list_in_x)
                        )
                    if l:
                        sc = d["å½“å‰æ•…äº‹èƒŒæ™¯"]
                        rp = d["å½“å‰æ•…äº‹èƒŒæ™¯"].replace(d["å‚ä¸è€…2"], l[0])
                        assert sc in out
                        out = out.replace(sc, rp)
                        req.append(out.strip())
                        req_iter = list(map(eval ,req))
                        req_iter = list(filter(lambda x: type(x) == type({}), req_iter))
                        yield "\n".join(map(lambda d:
                            "\n".join(map(lambda t2: "{}:{}".format(t2[0], t2[1]) ,d.items()))
                         ,req_iter))
        x = x.strip() + "\n" + out.strip()
    req_iter = list(map(eval ,req))
    req_iter = list(filter(lambda x: type(x) == type({}), req_iter))
    yield "\n".join(map(lambda d:
        "\n".join(map(lambda t2: "{}:{}".format(t2[0], t2[1]) ,d.items()))
     ,req_iter))

def get_global_background(title, content_option):
    if hasattr(title, "value"):
        title_ = title.value
    else:
        title_ = title
    if hasattr(content_option, "value"):
        content_option_ = content_option.value
    else:
        content_option_ = content_option
    plot_info_dict = find_plot_info(title_)
    #list(plot_info_dict.keys())
    #['æ•…äº‹èƒŒæ™¯', 'äº‹ä»¶èµ·å› ', 'äº‹ä»¶ç»è¿‡', 'äº‹ä»¶åè½¬', 'äº‹ä»¶ç»“æŸ', 'äº‹ä»¶æ„ä¹‰', 'åç»­å‰§æƒ…']
    Summary = plot_info_dict.get(content_option_, [""])[0]
    Summary = Summary.replace("\n\n", "\n")
    return Summary

def trigger_current_background(title ,Summary, current_background_num):
    if hasattr(title, "value"):
        title_ = title.value
    else:
        title_ = title
    if hasattr(Summary, "value"):
        Summary_ = Summary.value
    else:
        Summary_ = Summary
    if hasattr(current_background_num, "value"):
        current_background_num_ = current_background_num.value
    else:
        current_background_num_ = current_background_num
    current_background_num_ = int(current_background_num_)
    name_list_in_x = pick_names_from_summary(Summary_, name_list)
    #name_list_in_x
    instruction = build_instruction_for_plot_engine(title_ ,Summary_, name_list_in_x)
    #print(instruction)
    out_l_iter = run_step_infer_times_run(instruction, times = current_background_num_,
        name_list_in_x = name_list_in_x, temperature = 0.01)
    for ele in out_l_iter:
        yield ele

def trigger_selected_current_background(current_background, select_index, global_background):
    if hasattr(current_background, "value"):
        current_background_ = current_background.value
    else:
        current_background_ = current_background
    if hasattr(select_index, "value"):
        select_index_ = select_index.value
    else:
        select_index_ = select_index
    if hasattr(global_background, "value"):
        global_background_ = global_background.value
    else:
        global_background_ = global_background
    #select_index_ = int(select_index_)
    assert type(select_index_) == type("")

    l = current_background_.split("\n")
    req = []
    for i, ele in enumerate(l):
        i_res_3 = i % 3
        if not req:
            req.append([ele])
        else:
            if i_res_3 == 0:
                req.append([ele])
            elif i_res_3 == 1:
                req[-1].append(ele)
            else:
                req[-1].append(ele)
    req = list(filter(lambda x: len(x) == 3, req))
    if not req:
        return ["", "", ""]
    select_index_num = min(len(req) - 1, int(select_index_.split(":")[-1]))
    if select_index_.startswith("å½“å‰å¯¹è¯èƒŒæ™¯"):
        return list(map(lambda x: ":".join(x.split(":")[1:]) if ":" in x else x,
            req[select_index_num]
        ))
    else:
        t3 = list(map(lambda x: ":".join(x.split(":")[1:]) if ":" in x else x,
            req[select_index_num]
        ))
        t3[-1] = global_background_
        return t3

def model_chat(query: Optional[str], history: Optional[History],
    Person1:str, Person2: str, background:str, selected_current_background:str,
    role_want_to_play: str
) -> Tuple[str, str, History]:
    #system_head_messages = init_chat_system_messages(human_name, gpt_name, background)
    assert role_want_to_play in ['å¯¹è¯è€…1', 'å¯¹è¯è€…2']
    d = {}
    if role_want_to_play == "å¯¹è¯è€…1":
        d["å‚ä¸è€…1"] = Person1
        d["å‚ä¸è€…2"] = Person2
    else:
        d["å‚ä¸è€…1"] = Person2
        d["å‚ä¸è€…2"] = Person1
    system_head = build_chat_system_head(
        background, selected_current_background,
        d["å‚ä¸è€…1"], d["å‚ä¸è€…2"],
        character_portrait_df)
    system_head_messages = [
        {
            "role": "system",
            "content": system_head
        }
    ]
    assert type(system_head_messages) == type([])
    assert len(system_head_messages) == 1

    if query is None:
        query = ''
    if history is None:
        history = []

    #### drop empty system message head
    messages = history_to_messages(history, "")[1:]
    messages = system_head_messages + messages
    messages.append({'role': "user", 'content': query})

    print("messages :")
    print(messages)

    import re
    zh_pattern = u"[\u4e00-\u9fa5]+"

    req_iter = openai_predict_stream(messages, temperature = 0.5)
    for system, history in req_iter:
        if history:
            history[-1][-1] = history[-1][-1].replace(d["å‚ä¸è€…2"], "")
            zh_l = re.findall(zh_pattern, history[-1][-1])
            if zh_l:
                history[-1][-1] = history[-1][-1][history[-1][-1].find(zh_l[0]):]
        yield "", history

def re_model_chat(query: Optional[str], history: Optional[History],
    Person1:str, Person2: str, background:str, selected_current_background:str,
    role_want_to_play: str
) -> Tuple[str, str, History]:
    if len(history) >= 1:
        query, resp = history[-1]
        req_iter = model_chat(query, history[:-1], Person1, Person2, background, selected_current_background, role_want_to_play)
        for system, history in req_iter:
            yield "", history
    else:
        yield "", []


with gr.Blocks() as demo:
    gr.Markdown("""<center><font size=8>ğŸ˜ Genshin Impact Qwen-1.5-7B-Chat Plot Roleplay Turned âš¡ï¸ vLLM âš¡ï¸ Bot ğŸ”¥</center>""")

    with gr.Row():
        with gr.Column():
            with gr.Row():
                title = gr.Dropdown(choices = title_list,
                    label = "ğŸ’¡ç« èŠ‚æ ‡é¢˜",
                    interactive = True, value = title_list[0]
                )
            with gr.Row():
                content_option = gr.Radio(['æ•…äº‹èƒŒæ™¯', 'äº‹ä»¶èµ·å› ', 'äº‹ä»¶ç»è¿‡', 'äº‹ä»¶åè½¬', 'äº‹ä»¶ç»“æŸ', 'äº‹ä»¶æ„ä¹‰', 'åç»­å‰§æƒ…'],
                    label="âœ¨ç« èŠ‚å†…å®¹", interactive = True, value = "æ•…äº‹èƒŒæ™¯")
            with gr.Column():
                global_background_value = get_global_background(title, content_option)
                global_background = gr.Textbox(global_background_value,
                    label = "ğŸŒå…¨å±€å¯¹è¯èƒŒæ™¯ï¼ˆå¯ç¼–è¾‘ï¼‰", interactive = True)
            with gr.Column():
                current_background_num = gr.Slider(
                    1, 10, value=5, label="ç”Ÿæˆå½“å‰å¯¹è¯èƒŒæ™¯ä¸ªæ•°", step = 1
                )
                current_background_button = gr.Button("âš™ï¸é‡æ–°ç”Ÿæˆå½“å‰å¯¹è¯èƒŒæ™¯")
                current_background_value = list(trigger_current_background(title ,global_background, current_background_num))[-1]
                #print("current_background_value :", current_background_value)

                current_background = gr.Textbox(current_background_value,
                    label = "ğŸ–¼ï¸ç”Ÿæˆçš„å¯é€‰å½“å‰å¯¹è¯èƒŒæ™¯ï¼ˆå¯ç¼–è¾‘ï¼‰ï¼Œä»å³ä¾§è¿›è¡Œé€‰æ‹©é€‰ç”¨ã€‚", interactive = True)
        with gr.Column():
            with gr.Row():
                chat_index_list = []
                for i in range(10):
                    for prefix in ["å…¨å±€å¯¹è¯èƒŒæ™¯", "å½“å‰å¯¹è¯èƒŒæ™¯"]:
                        chat_index_list.append("{}:{}".format(prefix, i))

                chat_index = gr.Dropdown(choices = chat_index_list,
                    label = "âœï¸é€‰å®šå¯¹è¯ç´¢å¼•",
                    interactive = True, value = chat_index_list[0]
                )
                Person1_val, Person2_val, selected_current_background_value = trigger_selected_current_background(current_background, chat_index, global_background)
                Person1 = gr.Textbox(Person1_val,
                    label = "ğŸ¤­å¯¹è¯è€…1", interactive = False)
                Person2 = gr.Textbox(Person2_val,
                    label = "ğŸ˜Šå¯¹è¯è€…2", interactive = False)
                person_option = gr.Radio(['å¯¹è¯è€…1', 'å¯¹è¯è€…2'],
                    label="â˜‘ï¸ä½ æƒ³æ‰®æ¼”çš„å¯¹è¯è€…", interactive = True, value = "å¯¹è¯è€…1")
            with gr.Row():
                selected_current_background = gr.Textbox(selected_current_background_value,
                    label = "ğŸ“šé€‰å®šå¯¹è¯èƒŒæ™¯ï¼ˆå¯ç¼–è¾‘ï¼‰", interactive = True)

            chatbot = gr.Chatbot(
                label='svjack/Genshin_Impact_Qwen_1_5_Plot_Chat_roleplay_chat_AWQ/DPO_Genshin_Impact_Qwen_1_5_Plot_Engine_Step_Json_Short_AWQ',
                height = 768
                )
            textbox = gr.Textbox(lines=2, label='Input')

            with gr.Row():
                sumbit = gr.Button("ğŸš€ å‘é€")
            with gr.Row():
                re_sumbit = gr.Button("â™»ï¸ğŸš€ é‡æ–°å‘é€")
                clear_history = gr.Button("ğŸ§¹ æ¸…ç©ºå†å²")

    title.change(
        get_global_background,
        [title, content_option],
        global_background
    )
    content_option.change(
        get_global_background,
        [title, content_option],
        global_background
    )
    global_background.change(
        trigger_current_background,
        [title ,global_background, current_background_num],
        current_background
    )
    current_background_num.change(
        trigger_current_background,
        [title ,global_background, current_background_num],
        current_background
    )
    current_background_button.click(
        trigger_current_background,
        [title ,global_background, current_background_num],
        current_background
    )
    chat_index.change(
        trigger_selected_current_background,
        [current_background, chat_index, global_background],
        [Person1, Person2 ,selected_current_background]
    )
    current_background.change(
        trigger_selected_current_background,
        [current_background, chat_index, global_background],
        [Person1, Person2 ,selected_current_background]
    )

    sumbit.click(model_chat,
                 inputs=[textbox, chatbot, Person1, Person2,
                 global_background, selected_current_background, person_option],
                 outputs=[textbox, chatbot],
                 concurrency_limit = 100)
    re_sumbit.click(
                 re_model_chat,
                 inputs=[textbox, chatbot, Person1, Person2,
                 global_background, selected_current_background, person_option],
                 outputs=[textbox, chatbot],
                 concurrency_limit = 100)

    chat_index.change(
        fn=lambda _ : ("", []),
                            inputs=[],
                            outputs=[textbox, chatbot])
    current_background.change(
        fn=lambda _ : ("", []),
                            inputs=[],
                            outputs=[textbox, chatbot])
    selected_current_background.change(
        fn=lambda _ : ("", []),
                            inputs=[],
                            outputs=[textbox, chatbot])
    Person1.change(
        fn=lambda _ : ("", []),
                            inputs=[],
                            outputs=[textbox, chatbot])
    Person2.change(
        fn=lambda _ : ("", []),
                            inputs=[],
                            outputs=[textbox, chatbot])
    person_option.change(
        fn=lambda _ : ("", []),
                            inputs=[],
                            outputs=[textbox, chatbot])
    clear_history.click(fn=lambda _ : ("", []),
                        inputs=[],
                        outputs=[textbox, chatbot])

demo.queue(api_open=False)
demo.launch(max_threads=30, share = True)
