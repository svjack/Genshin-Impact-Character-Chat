'''
gradio==4.29.0
peft==0.11.1
transformers==4.41.0
bitsandbytes
huggingface_hub
datasets
Pillow==10.3.0
'''

import numpy as np
import pandas as pd
import gradio
import json
import os
import re

from PIL import Image
from threading import Thread

from transformers import TextStreamer, TextIteratorStreamer, AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

from huggingface_hub import snapshot_download

from typing import List, Optional, Tuple, Dict
History = List[Tuple[str, str]]
Messages = List[Dict[str, str]]

import gradio as gr

if not os.path.exists("genshin-impact-character"):
    path = snapshot_download(
        repo_id="svjack/genshin-impact-character",
        repo_type="dataset",
        local_dir="genshin-impact-character",
        local_dir_use_symlinks = False
    )

info_df = pd.read_csv("genshin-impact-character/genshin_impact_background_settings_constrained.csv")
info_df["info"] = info_df["info"].map(eval)

with open("genshin-impact-character/genshin_impact_character_setting.json", "r") as f:
    character_setting_total_dict = json.load(f)

req_dict = {}
for k, v_dict in character_setting_total_dict.items():
    req_dict[k] = {}
    for kk, vv in v_dict.items():
        if kk != "å…ƒç´ åŠ›":
            req_dict[k][kk] = vv
character_setting_total_dict = req_dict

def get_character_background_list(info_dict):
    text = []
    if "è§’è‰²è¯¦ç»†" in info_dict["æè¿°"]:
        text.append(info_dict["æè¿°"]["è§’è‰²è¯¦ç»†"])
    if "æ›´å¤šæè¿°" in info_dict["æè¿°"]:
        text.append(info_dict["æè¿°"]["æ›´å¤šæè¿°"])
    return list(map(lambda x: x.replace(" ", "").replace("\n\n", "\n"), text))
def get_character_background(info_dict, all = False):
    if all:
        return "\n".join(get_character_background_list(info_dict))
    else:
        return get_character_background_list(info_dict)[0] if get_character_background_list(info_dict) else ""

pd.DataFrame(
pd.Series(character_setting_total_dict.values()).map(
    lambda x: {
        "æ€§åˆ«": x['æ€§åˆ«'],
        "å›½ç±": x["å›½ç±"]
    }
).values.tolist()).apply(lambda x: set(x), axis = 0).to_dict()


character_setting_total_dist_dict = {
 'å§“å': "",
 'æ€§åˆ«': {'å°‘å¥³å¥³æ€§', 'å°‘å¹´ç”·æ€§', 'æˆå¹´å¥³æ€§', 'æˆå¹´ç”·æ€§'},
 'å›½ç±': {'æ«ä¸¹', 'ç’ƒæœˆ', 'ç¨»å¦»', 'è‡³å†¬', 'è’™å¾·', 'é¡»å¼¥'},
 'èº«ä»½': "",
 'æ€§æ ¼ç‰¹å¾': "",
 'è§’è‰²ä»‹ç»': "",
 }

def get_character_setting_total_dict(name):
    from copy import deepcopy
    req = deepcopy(character_setting_total_dist_dict)
    if name in character_setting_total_dict:
        for k, v in character_setting_total_dict[name].items():
            req[k] = v
        info_dict = dict(info_df[["title", "info"]].values.tolist())[name]
        req["è§’è‰²ä»‹ç»"] = get_character_background(info_dict)
    req["å§“å"] = name
    return req

prompt_format_dict = {
    "Basic_Info": ["æ€§åˆ«", "å›½ç±", "èº«ä»½", "æ€§æ ¼ç‰¹å¾"],

    "ä¸¤äººåŒå±{}": ["å›½ç±"],
    "{}æ¥è‡ª{},{}æ¥è‡ª{}ã€‚": ["å§“å", "å›½ç±", "å§“å", "å›½ç±"],

    "ä¸‹é¢æ˜¯{}çš„ä¸€äº›åŸºæœ¬ä¿¡æ¯\n{}": ["å§“å", "Basic_Info"],
    "ä¸‹é¢æ˜¯{}çš„ä¸€äº›åŸºæœ¬ä¿¡æ¯\n{}\nè¿™äº›æ˜¯ä¸€æ®µè§’è‰²ä»‹ç»\n{}": ["å§“å", "Basic_Info", "è§’è‰²ä»‹ç»"],

    "ç»­å†™ä¸‹é¢çš„è§’è‰²ä»‹ç»ï¼Œä¸‹é¢æ˜¯è§’è‰²ä»‹ç»çš„å¼€å¤´ã€‚{}æ˜¯{}ã€‚{}": ["å§“å", "èº«ä»½", "Text"],
    "ç»­å†™ä¸‹é¢çš„è§’è‰²æ•…äº‹ï¼Œä¸‹é¢æ˜¯è§’è‰²æ•…äº‹çš„å¼€å¤´ã€‚{}æ˜¯{}ã€‚{}": ["å§“å", "èº«ä»½", "Text"],
    "ç»­å†™ä¸‹é¢è·å¾—ç¥ä¹‹çœ¼çš„è¿‡ç¨‹ï¼Œä¸‹é¢æ˜¯å¼€å¤´ã€‚{}æ˜¯{}ã€‚{}": ["å§“å", "èº«ä»½", "Text"],
    "{}ç»™ä½ å†™äº†ä¸€å°ä¿¡ï¼Œä¿¡ä¸»é¢˜æ˜¯{}ï¼Œä¿¡çš„å†…å®¹æ˜¯è¿™æ ·çš„ã€‚": ["å§“å", "Text"],

    "{}åœ¨è¿›è¡Œæœ‰å…³{}çš„èŠå¤©æ—¶ä¼šè¯´ä»€ä¹ˆï¼Ÿ": ["å§“å", "Text"],
    "{}åœ¨{}çš„æ—¶å€™ä¼šè¯´ä»€ä¹ˆï¼Ÿ": ["å§“å", "Text"],
    "{}åœ¨{}æ—¶ä¼šè¯´ä»€ä¹ˆï¼Ÿ": ["å§“å", "Text"],
    "å…³äº{}ï¼Œ{}ä¼šè¯´ä»€ä¹ˆ?": ["Text", "å§“å"],
    "å½“ä½ æƒ³è¦äº†è§£{}æ—¶": ["å§“å"],

    "å…³äº{}ï¼Œ{}ä¼šè¯´ä»€ä¹ˆ?": ["å§“å", "å§“å"],
    "ä»{}é‚£é‡Œï¼Œå¯ä»¥è·å¾—å“ªäº›å…³äº{}çš„ä¿¡æ¯ï¼Ÿ": ["å§“å", "å§“å"]
}

def single_character_prompt_func(name,
    used_prompt_format_dict,
    character_setting_rewrite_dict = {},
    Text = "",
    ):
    assert type(used_prompt_format_dict) == type({})
    assert type(character_setting_rewrite_dict) == type({})
    character_setting_total_dict = get_character_setting_total_dict(name)
    for k, v in character_setting_rewrite_dict.items():
        if k in character_setting_total_dict:
            character_setting_total_dict[k] = v
    key = list(used_prompt_format_dict.keys())[0]
    assert key in prompt_format_dict
    if key == "Basic_Info":
        return "\n".join(
        map(lambda k: "{}:{}".format(k, character_setting_total_dict[k]), prompt_format_dict[key])
        )
    elif key == "ä¸¤äººåŒå±{}":
        return "ä¸¤äººåŒå±{}".format(character_setting_total_dict["å›½ç±"])
    elif key == "ä¸‹é¢æ˜¯{}çš„ä¸€äº›åŸºæœ¬ä¿¡æ¯\n{}":
        return "ä¸‹é¢æ˜¯{}çš„ä¸€äº›åŸºæœ¬ä¿¡æ¯\n{}".format(name,
            single_character_prompt_func(name,
                {
                    "Basic_Info": ["æ€§åˆ«", "å›½ç±", "èº«ä»½", "æ€§æ ¼ç‰¹å¾"]
                },
                character_setting_rewrite_dict
            )
        )
    elif key == "ä¸‹é¢æ˜¯{}çš„ä¸€äº›åŸºæœ¬ä¿¡æ¯\n{}\nè¿™äº›æ˜¯ä¸€æ®µè§’è‰²ä»‹ç»\n{}":
        return "ä¸‹é¢æ˜¯{}çš„ä¸€äº›åŸºæœ¬ä¿¡æ¯\n{}\nè¿™äº›æ˜¯ä¸€æ®µè§’è‰²ä»‹ç»\n{}".format(
            name,
            single_character_prompt_func(name,
                {
                    "Basic_Info": ["æ€§åˆ«", "å›½ç±", "èº«ä»½", "æ€§æ ¼ç‰¹å¾"]
                },
                character_setting_rewrite_dict
            ),
            character_setting_total_dict["è§’è‰²ä»‹ç»"]
        )
    elif key == "ç»­å†™ä¸‹é¢çš„è§’è‰²ä»‹ç»ï¼Œä¸‹é¢æ˜¯è§’è‰²ä»‹ç»çš„å¼€å¤´ã€‚{}æ˜¯{}ã€‚{}":
        return "ç»­å†™ä¸‹é¢çš„è§’è‰²ä»‹ç»ï¼Œä¸‹é¢æ˜¯è§’è‰²ä»‹ç»çš„å¼€å¤´ã€‚{}æ˜¯{}ã€‚{}".format(
            name,
            character_setting_total_dict["èº«ä»½"],
            Text
        )
    elif key == "ç»­å†™ä¸‹é¢çš„è§’è‰²æ•…äº‹ï¼Œä¸‹é¢æ˜¯è§’è‰²æ•…äº‹çš„å¼€å¤´ã€‚{}æ˜¯{}ã€‚{}":
        return "ç»­å†™ä¸‹é¢çš„è§’è‰²æ•…äº‹ï¼Œä¸‹é¢æ˜¯è§’è‰²ä»‹ç»çš„å¼€å¤´ã€‚{}æ˜¯{}ã€‚{}".format(
            name,
            character_setting_total_dict["èº«ä»½"],
            Text
        )
    elif key == "ç»­å†™ä¸‹é¢è·å¾—ç¥ä¹‹çœ¼çš„è¿‡ç¨‹ï¼Œä¸‹é¢æ˜¯å¼€å¤´ã€‚{}æ˜¯{}ã€‚{}":
        return "ç»­å†™ä¸‹é¢è·å¾—ç¥ä¹‹çœ¼çš„è¿‡ç¨‹ï¼Œä¸‹é¢æ˜¯å¼€å¤´ã€‚{}æ˜¯{}ã€‚{}".format(
            name,
            character_setting_total_dict["èº«ä»½"],
            Text
        )
    elif key == "{}ç»™ä½ å†™äº†ä¸€å°ä¿¡ï¼Œä¿¡ä¸»é¢˜æ˜¯{}ï¼Œä¿¡çš„å†…å®¹æ˜¯è¿™æ ·çš„ã€‚":
        return "{}ç»™ä½ å†™äº†ä¸€å°ä¿¡ï¼Œä¿¡ä¸»é¢˜æ˜¯{}ï¼Œä¿¡çš„å†…å®¹æ˜¯è¿™æ ·çš„ã€‚".format(
            name,
            Text
        )
    elif key == "{}åœ¨è¿›è¡Œæœ‰å…³{}çš„èŠå¤©æ—¶ä¼šè¯´ä»€ä¹ˆï¼Ÿ":
        return "{}åœ¨è¿›è¡Œæœ‰å…³{}çš„èŠå¤©æ—¶ä¼šè¯´ä»€ä¹ˆï¼Ÿ".format(
            name,
            Text
        )
    elif key == "{}åœ¨{}çš„æ—¶å€™ä¼šè¯´ä»€ä¹ˆï¼Ÿ":
        return "{}åœ¨{}çš„æ—¶å€™ä¼šè¯´ä»€ä¹ˆï¼Ÿ".format(
            name,
            Text
        )
    elif key == "{}åœ¨{}æ—¶ä¼šè¯´ä»€ä¹ˆï¼Ÿ":
        return "{}åœ¨{}æ—¶ä¼šè¯´ä»€ä¹ˆï¼Ÿ".format(
            name,
            Text
        )
    elif key == "å…³äº{}ï¼Œ{}ä¼šè¯´ä»€ä¹ˆ?":
        return "å…³äº{}ï¼Œ{}ä¼šè¯´ä»€ä¹ˆ?".format(
            Text,
            name,
        )
    elif key == "å½“ä½ æƒ³è¦äº†è§£{}æ—¶":
        return "å½“ä½ æƒ³è¦äº†è§£{}æ—¶".format(
            name,
        )
    return 1 / 0

def two_character_prompt_func(
    name_1,
    name_2,
    used_prompt_format_dict,
    character_setting_rewrite_dict_1 = {},
    character_setting_rewrite_dict_2 = {},
    ):
    assert type(character_setting_rewrite_dict_1) == type({})
    character_setting_total_dict_1 = get_character_setting_total_dict(name_1)
    for k, v in character_setting_rewrite_dict_1.items():
        if k in character_setting_total_dict_1:
            character_setting_total_dict_1[k] = v
    character_setting_total_dict_2 = get_character_setting_total_dict(name_2)
    for k, v in character_setting_rewrite_dict_2.items():
        if k in character_setting_total_dict_2:
            character_setting_total_dict_2[k] = v
    key = list(used_prompt_format_dict.keys())[0]
    assert key in prompt_format_dict
    if key == "å…³äº{}ï¼Œ{}ä¼šè¯´ä»€ä¹ˆ?":
        return "å…³äº{}ï¼Œ{}ä¼šè¯´ä»€ä¹ˆ?".format(name_1, name_2)
    elif key == "ä»{}é‚£é‡Œï¼Œå¯ä»¥è·å¾—å“ªäº›å…³äº{}çš„ä¿¡æ¯ï¼Ÿ":
        return "ä»{}é‚£é‡Œï¼Œå¯ä»¥è·å¾—å“ªäº›å…³äº{}çš„ä¿¡æ¯ï¼Ÿ".format(name_1, name_2)
    elif key == "{}æ¥è‡ª{},{}æ¥è‡ª{}ã€‚":
        return "{}æ¥è‡ª{},{}æ¥è‡ª{}ã€‚".format(name_1, character_setting_total_dict_1["å›½ç±"],
        name_2, character_setting_total_dict_2["å›½ç±"],
        )
    return 1 / 0

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen1.5-7B-Chat",)
qw_model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen1.5-7B-Chat", load_in_4bit = True)
qw_model = PeftModel.from_pretrained(qw_model,
                "svjack/Genshin_Impact_Qwen_1_5_Chat_sharegpt_roleplay_chat_lora_small"
        )
qw_model = qw_model.eval()
streamer = TextStreamer(tokenizer)
iter_streamer = TextIteratorStreamer(tokenizer)

def qwen_hf_predict(messages, qw_model = qw_model,
    tokenizer = tokenizer, streamer = streamer,
    do_sample = True,
    top_p = 0.95,
    top_k = 40,
    max_new_tokens = 2070,
    max_input_length = 3500,
    temperature = 0.9,
    repetition_penalty = 1.0,
    device = "cuda"):

    encodeds = tokenizer.apply_chat_template(messages, return_tensors="pt",
        add_generation_prompt=True
    )
    model_inputs = encodeds.to(device)

    generated_ids = qw_model.generate(model_inputs, max_new_tokens=max_new_tokens,
                                do_sample=do_sample,
                                  streamer = streamer,
                                  top_p = top_p,
                                  top_k = top_k,
                                  temperature = temperature,
                                  repetition_penalty = repetition_penalty,
                                  )
    out = tokenizer.batch_decode(generated_ids)[0].split("<|im_start|>assistant")[-1].replace("<|im_end|>", "").strip()
    return out

#### 1 - Answer questions about Genshin Impact Character using Third Person.
def third_person_instruction(single_name, question):
    assert type(single_name) == type("")
    assert type(question) == type("")
    info_prompt = single_character_prompt_func(
        single_name,
        {
        "ä¸‹é¢æ˜¯{}çš„ä¸€äº›åŸºæœ¬ä¿¡æ¯\n{}\nè¿™äº›æ˜¯ä¸€æ®µè§’è‰²ä»‹ç»\n{}": ["å§“å", "Basic_Info", "è§’è‰²ä»‹ç»"]
        },
    )
    messages=[
                    {
                        "role": "system",
                        "content": "",
                    },
                    {
                        "role": "user",
                        "content": '''
                        {info_prompt}
                        {question}
                        '''.format(**{
                            "info_prompt": info_prompt,
                            "question": question
                        })
                    }
    ]
    return messages

#### 2 - Answer questions about Genshin Impact Character using the First Person.
def first_person_instruction(single_name, question):
    assert type(single_name) == type("")
    assert type(question) == type("")
    info_prompt = single_character_prompt_func(
        single_name,
        {
        "ä¸‹é¢æ˜¯{}çš„ä¸€äº›åŸºæœ¬ä¿¡æ¯\n{}\nè¿™äº›æ˜¯ä¸€æ®µè§’è‰²ä»‹ç»\n{}": ["å§“å", "Basic_Info", "è§’è‰²ä»‹ç»"]
        },
    )
    messages=[
                    {
                        "role": "system",
                        "content": '''
                        äººç‰©è®¾å®š:
                        {info_prompt}

                        ä½ æ‰®æ¼”:{single_name}
                        '''.format(**{
                            "info_prompt": info_prompt,
                            "single_name": single_name
                        })
                    },
                    {
                        "role": "user",
                        "content": question
                    }
    ]
    return messages

##### 4 - Give Story Background between Genshin Impact Characters
def init_background_instruction(single_name_1, single_name_2):
    assert type(single_name_1) == type("")
    assert type(single_name_2) == type("")
    info_prompt_1 = single_character_prompt_func(
        single_name_1,
        {
        "ä¸‹é¢æ˜¯{}çš„ä¸€äº›åŸºæœ¬ä¿¡æ¯\n{}\nè¿™äº›æ˜¯ä¸€æ®µè§’è‰²ä»‹ç»\n{}": ["å§“å", "Basic_Info", "è§’è‰²ä»‹ç»"]
        },
    )
    info_prompt_2 = single_character_prompt_func(
        single_name_2,
        {
        "ä¸‹é¢æ˜¯{}çš„ä¸€äº›åŸºæœ¬ä¿¡æ¯\n{}\nè¿™äº›æ˜¯ä¸€æ®µè§’è‰²ä»‹ç»\n{}": ["å§“å", "Basic_Info", "è§’è‰²ä»‹ç»"]
        },
    )
    character_setting_total_dict_1 = get_character_setting_total_dict(single_name_1)
    character_setting_total_dict_2 = get_character_setting_total_dict(single_name_2)
    country_prompt = ""
    same_country = character_setting_total_dict_1["å›½ç±"] == character_setting_total_dict_2["å›½ç±"]
    if same_country:
        country_prompt = single_character_prompt_func(
            single_name_1,
            {
                "ä¸¤äººåŒå±{}": ["å›½ç±"]
            },
            )
    else:
        country_prompt = two_character_prompt_func(
                single_name_1,
                single_name_2,
                {
                "{}æ¥è‡ª{},{}æ¥è‡ª{}ã€‚": ["å§“å", "å›½ç±", "å§“å", "å›½ç±"]
                },
            )
    messages=[
                    {
                            "role": "system",
                            "content": "",
                    },
                    {
                        "role": "user",
                        "content": '''
                        äººç‰©è®¾å®š:
                        {info_prompt_1}
                        {info_prompt_2}
                        {country_prompt}

                        æ ¹æ®ä¸Šé¢çš„äººç‰©è®¾å®šç”Ÿæˆå‘ç”Ÿåœ¨{single_name_1}å’Œ{single_name_2}ä¹‹é—´çš„æ•…äº‹èƒŒæ™¯
                        '''.format(**{
                            "info_prompt_1": info_prompt_1,
                            "info_prompt_2": info_prompt_2,
                            "country_prompt": country_prompt,
                            "single_name_1": single_name_1,
                            "single_name_2": single_name_2
                        })
                    }
    ]
    return messages


#### 5 - Role Play Chat with Character Agent, provide your own character setting as another Character At same time, provide above Story Background, as Conversation background.
def init_chat_system_messages(human_name, gpt_name, background):
    assert type(human_name) == type("")
    assert type(gpt_name) == type("")
    assert type(background) == type("")
    info_prompt_1 = single_character_prompt_func(
        human_name,
        {
        "ä¸‹é¢æ˜¯{}çš„ä¸€äº›åŸºæœ¬ä¿¡æ¯\n{}\nè¿™äº›æ˜¯ä¸€æ®µè§’è‰²ä»‹ç»\n{}": ["å§“å", "Basic_Info", "è§’è‰²ä»‹ç»"]
        },
    )
    info_prompt_2 = single_character_prompt_func(
        gpt_name,
        {
        "ä¸‹é¢æ˜¯{}çš„ä¸€äº›åŸºæœ¬ä¿¡æ¯\n{}\nè¿™äº›æ˜¯ä¸€æ®µè§’è‰²ä»‹ç»\n{}": ["å§“å", "Basic_Info", "è§’è‰²ä»‹ç»"]
        },
    )
    character_setting_total_dict_1 = get_character_setting_total_dict(human_name)
    character_setting_total_dict_2 = get_character_setting_total_dict(gpt_name)
    country_prompt = ""
    same_country = character_setting_total_dict_1["å›½ç±"] == character_setting_total_dict_2["å›½ç±"]
    if same_country:
        country_prompt = single_character_prompt_func(
            human_name,
            {
                "ä¸¤äººåŒå±{}": ["å›½ç±"]
            },
            )
    else:
        country_prompt = two_character_prompt_func(
                human_name,
                gpt_name,
                {
                "{}æ¥è‡ª{},{}æ¥è‡ª{}ã€‚": ["å§“å", "å›½ç±", "å§“å", "å›½ç±"]
                },
            )
    messages=[
                    {
                        "role": "system",
                        "content": '''
                        äººç‰©è®¾å®š:
                        {info_prompt_1}
                        {info_prompt_2}
                        {country_prompt}

                        èƒŒæ™¯è®¾å®š:
                        {background}

                        ä½ æ‰®æ¼”:{gpt_name}
                        '''.format(**{
                            "info_prompt_1": info_prompt_1,
                            "info_prompt_2": info_prompt_2,
                            "country_prompt": country_prompt,
                            "background": background,
                            "gpt_name": gpt_name
                        })
                    }
    ]
    return messages

#### 6 - Generate New Story Background, on previous Background and Chat Context.
def new_background_instruction(human_name, gpt_name, previous_background,
    hist_chat_messages
):
    assert type(human_name) == type("")
    assert type(gpt_name) == type("")
    assert type(previous_background) == type("")
    assert type(hist_chat_messages) == type([])
    info_prompt_1 = single_character_prompt_func(
        human_name,
        {
        "ä¸‹é¢æ˜¯{}çš„ä¸€äº›åŸºæœ¬ä¿¡æ¯\n{}\nè¿™äº›æ˜¯ä¸€æ®µè§’è‰²ä»‹ç»\n{}": ["å§“å", "Basic_Info", "è§’è‰²ä»‹ç»"]
        },
    )
    info_prompt_2 = single_character_prompt_func(
        gpt_name,
        {
        "ä¸‹é¢æ˜¯{}çš„ä¸€äº›åŸºæœ¬ä¿¡æ¯\n{}\nè¿™äº›æ˜¯ä¸€æ®µè§’è‰²ä»‹ç»\n{}": ["å§“å", "Basic_Info", "è§’è‰²ä»‹ç»"]
        },
    )
    character_setting_total_dict_1 = get_character_setting_total_dict(human_name)
    character_setting_total_dict_2 = get_character_setting_total_dict(gpt_name)
    country_prompt = ""
    same_country = character_setting_total_dict_1["å›½ç±"] == character_setting_total_dict_2["å›½ç±"]
    if same_country:
        country_prompt = single_character_prompt_func(
            human_name,
            {
                "ä¸¤äººåŒå±{}": ["å›½ç±"]
            },
            )
    else:
        country_prompt = two_character_prompt_func(
                human_name,
                gpt_name,
                {
                "{}æ¥è‡ª{},{}æ¥è‡ª{}ã€‚": ["å§“å", "å›½ç±", "å§“å", "å›½ç±"]
                },
            )
    hist_chat_messages_ = list(
        filter(lambda d: d["role"] in ["user", "assistant"], hist_chat_messages)
    )
    chat_context = "\n".join(map(lambda d: "{}:{}".format(
    human_name if d["role"] == "user" else gpt_name ,d["content"]
    ), hist_chat_messages_))
    messages=[
                    {
                        "role": "system",
                        "content": "",
                    },
                    {
                        "role": "user",
                        "content": '''
                        äººç‰©è®¾å®š:
                        {info_prompt_1}
                        {info_prompt_2}
                        {country_prompt}

                        ä¸‹é¢æ˜¯å‘ç”Ÿåœ¨{human_name}å’Œ{gpt_name}ä¹‹é—´çš„æ•…äº‹èƒŒæ™¯:
                        {previous_background}

                        äºŒäººå‘ç”Ÿäº†å¦‚ä¸‹å¯¹è¯:
                        {chat_context}

                        åŒæ—¶ï¼Œä¸ºæ¨åŠ¨å¯¹è¯æƒ…èŠ‚å‘å±•ï¼Œè¯·ä½ ç”¨ç±»ä¼¼ä¸Šé¢æ•…äº‹èƒŒæ™¯çš„é£æ ¼ï¼Œç»™å‡ºä¸€ä¸ªåŸºäºä¸Šé¢è®¾å®šçš„æ–°æ•…äº‹èƒŒæ™¯ï¼Œè¦æ±‚æ–°æ•…äº‹èƒŒæ™¯ä¸åŸæ•…äº‹èƒŒæ™¯æœ‰å› æœè”ç³»ã€‚
                        ä½¿å¾—{human_name}å’Œ{gpt_name}å¯ä»¥åœ¨æ–°çš„æ•…äº‹èƒŒæ™¯ä¸­è¿›è¡Œäº’åŠ¨ã€‚
                        è¦æ±‚åªè¾“å‡ºä¸€è¡Œæ–‡å­—ï¼Œæ–°æ•…äº‹èƒŒæ™¯ä¸­å¿…é¡»æåˆ°{human_name}å’Œ{gpt_name}ã€‚
                        '''.format(**{
                            "info_prompt_1": info_prompt_1,
                            "info_prompt_2": info_prompt_2,
                            "country_prompt": country_prompt,
                            "previous_background":previous_background,
                            "chat_context": chat_context,
                            "human_name": human_name,
                            "gpt_name": gpt_name
                        })
                    }
    ]
    return messages

def history_to_messages(history: History, system: str) -> Messages:
    messages = [{'role': "system", 'content': system}]
    for h in history:
        messages.append({'role': "user", 'content': h[0]})
        messages.append({'role': "assistant", 'content':
            h[1]
        })
    return messages

def messages_to_history(messages: Messages) -> Tuple[str, History]:
    assert messages[0]['role'] == "system"
    system = messages[0]['content']
    history = []
    import numpy as np
    import pandas as pd
    from copy import deepcopy
    messages = deepcopy(messages)

    messages_ = []
    for ele in messages[1:]:
        if not messages_:
            messages_.append(ele)
        else:
            if messages_[-1]["role"] == ele["role"]:
                continue
            else:
                messages_.append(ele)

    last_message = messages_[-1]
    last_role = last_message["role"]
    if last_role == "user":
        messages_.append(
            {
                "role": "assistant",
                "content": ""
            }
        )
    history = pd.DataFrame(np.asarray(messages_).reshape([-1, 2]).tolist()).applymap(
        lambda x: x["content"]
    ).applymap(
        lambda x: x
    ).values.tolist()
    return system, history

def qwen_hf_predict_stream(messages, qw_model = qw_model,
    tokenizer = tokenizer, streamer = iter_streamer,
    do_sample = True,
    top_p = 0.95,
    top_k = 40,
    max_new_tokens = 2070,
    max_input_length = 3500,
    temperature = 0.9,
    repetition_penalty = 1.0,
    device = "cuda"):

    encodeds = tokenizer.apply_chat_template(messages, return_tensors="pt",
        add_generation_prompt=True
    )
    model_inputs = encodeds.to(device)

    def generate_and_signal_complete():
        qw_model.generate(model_inputs, max_new_tokens=max_new_tokens,
                                    do_sample=do_sample,
                                      streamer = streamer,
                                      top_p = top_p,
                                      top_k = top_k,
                                      temperature = temperature,
                                      repetition_penalty = repetition_penalty,
                                      )

    t1 = Thread(target=generate_and_signal_complete)
    t1.start()

    system, history = messages_to_history(messages)

    # Initialize an empty string to store the generated text
    partial_text = ""
    for new_text in streamer:
        partial_text += new_text.split("<|im_start|>assistant")[-1].replace("<|im_end|>", "").strip()
        history[-1][1] = partial_text
        yield system, history
    #out = tokenizer.batch_decode(generated_ids)[0].split("<|im_start|>assistant")[-1].replace("<|im_end|>", "").strip()

#### return background iter
def init_background_chat(single_name_1, single_name_2):
    if hasattr(single_name_1, "value"):
        single_name_1_ = single_name_1.value
    else:
        single_name_1_ = single_name_1
    if hasattr(single_name_2, "value"):
        single_name_2_ = single_name_2.value
    else:
        single_name_2_ = single_name_2

    messages = init_background_instruction(single_name_1_, single_name_2_)
    req_iter = qwen_hf_predict_stream(messages)
    for system, history in req_iter:
        yield history[-1][1]

#### return new background iter
def new_background_chat(human_name:str, gpt_name:str, previous_background:str,
    history: Optional[History]):

    if history is None:
        history = []
    #### drop empty system message head
    hist_chat_messages = history_to_messages(history, "")[1:]

    messages = new_background_instruction(human_name, gpt_name, previous_background,
        hist_chat_messages)
    req_iter = qwen_hf_predict_stream(messages)
    for system, history in req_iter:
        yield history[-1][1]

#### return system hist iter
def model_chat(query: Optional[str], history: Optional[History],
    human_name: str, gpt_name: str, background: str
) -> Tuple[str, str, History]:
    system_head_messages = init_chat_system_messages(human_name, gpt_name, background)
    assert type(system_head_messages) == type([])
    assert len(system_head_messages) == 1

    if query is None:
        query = ''
    if history is None:
        history = []

    #### drop empty system message head
    messages = history_to_messages(history, "")[1:]
    messages = system_head_messages + messages
    messages.append({'role': "user", 'content': query})

    req_iter = qwen_hf_predict_stream(messages)
    for system, history in req_iter:
        yield "", history

with gr.Blocks() as demo:
    gr.Markdown("""<center><font size=8>ğŸ˜Š Genshin Impact Qwen-1.5-7B-Chat Sharegpt Roleplay Turned Transformer Bot ğŸ”¥</center>""")

    with gr.Row():
        human_name = gr.Dropdown(choices = info_df["title"].values.tolist(), label = "ğŸ©ä½ çš„è§’è‰²",
            interactive = True, value = "ä¸½è"
        )
        gpt_name = gr.Dropdown(choices = info_df["title"].values.tolist(), label = "ğŸ¤–æœºå™¨äººè§’è‰²",
            interactive = True, value = "æçº³é‡Œ"
        )
    with gr.Row():
        background_value = list(init_background_chat(human_name, gpt_name))[-1]
        background = gr.Textbox(background_value ,
            label = "ğŸ–¼ï¸å¯¹è¯èƒŒæ™¯ï¼ˆå¯ç¼–è¾‘ï¼‰", interactive = True, lines = 2)

    chatbot = gr.Chatbot(label='svjack/Genshin_Impact_Qwen_1_5_Chat_sharegpt_roleplay_chat_lora_small')
    textbox = gr.Textbox(lines=2, label='Input')

    with gr.Row():
        clear_history = gr.Button("ğŸ§¹ æ¸…ç©ºå†å²")
        sumbit = gr.Button("ğŸš€ å‘é€")
    with gr.Row():
        reset_background = gr.Button("â™»ï¸é‡ç½®å¯¹è¯èƒŒæ™¯")
        new_background = gr.Button("â¡ï¸æ¨è¿›å¯¹è¯èƒŒæ™¯")

    sumbit.click(model_chat,
                 inputs=[textbox, chatbot, human_name, gpt_name, background],
                 outputs=[textbox, chatbot],
                 concurrency_limit = 100)
    clear_history.click(fn=lambda _ : ("", []),
                        inputs=[],
                        outputs=[textbox, chatbot])

    reset_background.click(
        init_background_chat,
        inputs = [human_name, gpt_name],
        outputs = background
    )
    new_background.click(
        new_background_chat,
        inputs = [human_name, gpt_name, background, chatbot],
        outputs = background
    )

    human_name.change(
        init_background_chat,
        inputs = [human_name, gpt_name],
        outputs = background
    )
    human_name.change(fn=lambda _ : ("", []),
                        inputs=[],
                        outputs=[textbox, chatbot])
    gpt_name.change(
        init_background_chat,
        inputs = [human_name, gpt_name],
        outputs = background
    )
    gpt_name.change(fn=lambda _ : ("", []),
                        inputs=[],
                        outputs=[textbox, chatbot])

demo.queue(api_open=False)
demo.launch(max_threads=30, share = True)
